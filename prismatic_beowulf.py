# -*- coding: utf-8 -*-
"""Prismatic Beowulf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBMMi-DtJ-NIrwEGYnpnWOwPi4cJaSnA

#Pre-processing
"""

input = "/content/input"

import re
import os

# Define a function to clean text
def clean_text(text):
    # Remove newline characters (line breaks)
    cleaned_text = text.replace('\n', '  ')

    return cleaned_text

# Iterate through the text files in the output folder
for text_filename in os.listdir(input):
    if text_filename.endswith('.txt'):
        text_file_path = os.path.join(input, text_filename)

        # Read the text file
        with open(text_file_path, 'r', encoding='utf-8') as text_file:
            text_data = text_file.read()

        # Clean the text data
        cleaned_text_data = clean_text(text_data)

        # Write the cleaned data back to the text file
        with open(text_file_path, 'w', encoding='utf-8') as text_file:
            text_file.write(cleaned_text_data)

print("Text data cleaning completed.")

pip install vtk

import vtk

def writeNodesEdges(nodeCoords,
                 scalar = [], name = [], power = [1,1],
                 nodeLabel = [],
                 edges = [],
                 method = 'vtkPolyData',
                 fileout = 'test'):
    """
    Store points and/or graphs as vtkPolyData or vtkUnstructuredGrid.
    Required argument:
    - nodeCoords is an array of node coordinates (nnodes,3)
    Optional arguments:
    - edges is a list of edges in the format [nodeID1,nodeID2]
    - scalar is the list of attributes, each is the list of scalars for all nodes
    - name is the list of scalars' names
    - power is the scaling list for attributes: 1 for r~scalars, 0.333 for V~scalars
    - nodeLabel is a list of node labels
    - method = 'vtkPolyData' or 'vtkUnstructuredGrid'
    - fileout is the output file name (will be given .vtp or .vtu extension)
    """

    points = vtk.vtkPoints()
    for node in nodeCoords:
        points.InsertNextPoint(node)

    if edges:
        line = vtk.vtkCellArray()
        line.Allocate(len(edges))
        for edge in edges:
            line.InsertNextCell(2)
            line.InsertCellPoint(edge[0])
            line.InsertCellPoint(edge[1])   # line from point edge[0] to point edge[1]

    if scalar:
        attribute0 = vtk.vtkFloatArray()
        attribute0.SetNumberOfComponents(1)
        attribute0.SetName(name[0])
        attribute0.SetNumberOfTuples(len(scalar[0]))
        for i, j in enumerate(scalar[0]):   # i becomes 0,1,2,..., and j runs through scalar[0]
            attribute0.SetValue(i,j**power[0])
        if len(scalar) > 1:
            attribute1 = vtk.vtkFloatArray()
            attribute1.SetNumberOfComponents(1)
            attribute1.SetName(name[1])
            attribute1.SetNumberOfTuples(len(scalar[1]))
            for i, j in enumerate(scalar[1]):   # i becomes 0,1,2,..., and j runs through scalar[1]
                attribute1.SetValue(i,j**power[1])

    if nodeLabel:
        label = vtk.vtkStringArray()
        label.SetName('tag')
        label.SetNumberOfValues(len(nodeLabel))
        for i, j in enumerate(nodeLabel):   # i becomes 0,1,2,..., and j runs through scalar
            label.SetValue(i,j)

    if method == 'vtkPolyData':
        polydata = vtk.vtkPolyData()
        polydata.SetPoints(points)
        if edges:
            polydata.SetLines(line)
        if scalar:
            polydata.GetPointData().AddArray(attribute0)
            if len(scalar) > 1:
                polydata.GetPointData().AddArray(attribute1)
        if nodeLabel:
            polydata.GetPointData().AddArray(label)
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(fileout+'.vtp')
        writer.SetInputData(polydata)
        writer.Write()
        print('wrote into', fileout+'.vtp')
    elif method == 'vtkUnstructuredGrid':
        # caution: ParaView's Tube filter does not work on vtkUnstructuredGrid
        grid = vtk.vtkUnstructuredGrid()
        grid.SetPoints(points)
        if edges:
            grid.SetCells(vtk.VTK_LINE, line)
        if scalar:
            grid.GetPointData().AddArray(attribute0)
            if len(scalar) > 1:
                grid.GetPointData().AddArray(attribute1)
        if nodeLabel:
            grid.GetPointData().AddArray(label)
        writer = vtk.vtkXMLUnstructuredGridWriter()
        writer.SetFileName(fileout+'.vtu')
        writer.SetInputData(grid)
        writer.Write()
        print('wrote into', fileout+'.vtu')

#Import libraries for vectorialisation
import string
from gensim import corpora
from collections import defaultdict
from scipy.spatial import distance
import numpy as np
from sklearn import manifold
'''from nodesAndEdges import writeNodesEdges, readLongestParagraphs'''
from tqdm import tqdm

"""## Vectorise Longest Paragraphs"""

import os

# Define a function to read the longest paragraphs from a file
def readLongestParagraphs(filenames, size, numberParagraphs=None):
    longest_paragraphs = []

    for filename in filenames:
        with open(filename, 'r', encoding='utf-8') as f:
            data = f.read()

        # Split text into paragraphs
        paragraphs = data.split("\n\n")

        # Only keep paragraphs longer than 10 words
        paragraphs = [p for p in paragraphs if len(p) > 10]

        # Pick numberParagraphs longest paragraphs for analysis
        longest = sorted(paragraphs, key=len, reverse=True)
        actualNumberParagraphs = min(len(longest), numberParagraphs)
        print("Read", actualNumberParagraphs, "paragraphs from", filename)
        size.append(actualNumberParagraphs)
        longest_paragraphs.extend(longest[:numberParagraphs])

    return longest_paragraphs

# Define the folder where your documents are located
folder_path = 'C:\\Users\\David\\Python_Projects\\Fantasticland_visualised\Chapters\chapters_txt'
# Specify the actual number of paragraphs per text
size = []
# Specify the target number of paragraphs per text
npar = 30

# Create a list of file paths
filenames = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(".txt")]

documents = readLongestParagraphs(filenames, size, npar)

# Loop through all the files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith(".txt"):  # You can specify the file extension you want to process
        file_path = os.path.join(folder_path, filename)

CharacterTags = list(range(1, 25))
ChaptersPerCharacterTags = list(range(1, 25))

# convert line breaks and dashes to spaces, and remove punctuation
for i, p in enumerate(documents):
    tmp = p.replace('\n', ' ').replace('-',' ')
    for c in string.punctuation:
        tmp = tmp.replace(c,'')
    documents[i] = tmp

# remove common words and tokenize (break into words)
stoplist = set('for from a of the and to in at through'.split())
texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]

# count words across all paragraphs
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1

# remove words that appear only once across all paragraphs
texts = [[token for token in text if frequency[token] > 1] for text in texts]

# build a dictionary of words from scratch (not related to above word count)
dictionary = corpora.Dictionary(texts)
# print(dictionary.token2id)   # print IDs of all words in the dictionary
nwords = len(dictionary.token2id)
print('built a global dictionary with', nwords, 'words')

# convert documents to sparse vectors containing tuples (wordID, wordCount);
# corpus is a list of paragraphs, each is a list of tuples
corpus = [dictionary.doc2bow(text) for text in texts]

# convert sparse vectors to full vectors of length nwords
n = sum(size)
fullCorpus = np.zeros((n,nwords), dtype=np.int32)
for i, d in enumerate(corpus):
    for word in d:
        id, count = word
        fullCorpus[i,id] = count

# connect pairs with at least 3 words in common
n, i = len(fullCorpus), -1
edges = []
for d1 in tqdm(fullCorpus):
    i += 1
    row = []
    for j, d2 in enumerate(fullCorpus):
        if i < j:
            if sum((d1!=0) * (d2!=0)) >=5:
                edges.append([i,j])

author, novelPerAuthor = [], []
for i, s in enumerate(size):
    author += [CharacterTags[i]] * s
    novelPerAuthor += [ChaptersPerCharacterTags[i]] * s

import networkx as nx
H = nx.Graph()
H.add_nodes_from(range(n))
H.add_edges_from(edges)
pos = nx.spring_layout(H, k=1.2, dim=3)   # dictionary (nodeID,array([x,y,z]))
xyz = np.zeros((n,3))
for i in pos:
    xyz[i,:] = pos[i]

print(nx.number_of_nodes(H), 'nodes and', nx.number_of_edges(H), 'edges')

print(xyz)
writeNodesEdges(xyz, edges=edges, scalar=[author,novelPerAuthor],
             name=['author','novel per author'], power=[1,0.7], fileout='network5')

"""#Beowulf Project"""

pip install vtk

import vtk

def readLongestParagraphs(filename, size, numberParagraphs=None):

    f = open(filename, 'r')
    data = f.read()

    # split text into paragraphs
    paragraphs = data.split("\n\n")

    # only keep paragraphs longer than 10 words
    paragraphs = [p for p in paragraphs if len(p)>10]

    # pick numberParagraphs longest paragraphs for analysis
    longest = sorted(paragraphs, key=len, reverse=True)
    actualNumberParagraphs = min(len(longest), numberParagraphs)
    print("read", actualNumberParagraphs, "paragraphs from", filename)
    size.append(actualNumberParagraphs)
    return longest[0:numberParagraphs]

def writeNodesEdges(nodeCoords,
                 scalar = [], name = [], power = [1,1],
                 nodeLabel = [],
                 edges = [],
                 method = 'vtkPolyData',
                 fileout = 'test'):

    points = vtk.vtkPoints()
    for node in nodeCoords:
        points.InsertNextPoint(node)

    if edges:
        line = vtk.vtkCellArray()
        line.Allocate(len(edges))
        for edge in edges:
            line.InsertNextCell(2)
            line.InsertCellPoint(edge[0])
            line.InsertCellPoint(edge[1])   # line from point edge[0] to point edge[1]

    if scalar:
        attribute0 = vtk.vtkFloatArray()
        attribute0.SetNumberOfComponents(1)
        attribute0.SetName(name[0])
        attribute0.SetNumberOfTuples(len(scalar[0]))
        for i, j in enumerate(scalar[0]):   # i becomes 0,1,2,..., and j runs through scalar[0]
            attribute0.SetValue(i,j**power[0])
        if len(scalar) > 1:
            attribute1 = vtk.vtkFloatArray()
            attribute1.SetNumberOfComponents(1)
            attribute1.SetName(name[1])
            attribute1.SetNumberOfTuples(len(scalar[1]))
            for i, j in enumerate(scalar[1]):   # i becomes 0,1,2,..., and j runs through scalar[1]
                attribute1.SetValue(i,j**power[1])

    if nodeLabel:
        label = vtk.vtkStringArray()
        label.SetName('tag')
        label.SetNumberOfValues(len(nodeLabel))
        for i, j in enumerate(nodeLabel):   # i becomes 0,1,2,..., and j runs through scalar
            label.SetValue(i,j)

    if method == 'vtkPolyData':
        polydata = vtk.vtkPolyData()
        polydata.SetPoints(points)
        if edges:
            polydata.SetLines(line)
        if scalar:
            polydata.GetPointData().AddArray(attribute0)
            if len(scalar) > 1:
                polydata.GetPointData().AddArray(attribute1)
        if nodeLabel:
            polydata.GetPointData().AddArray(label)
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(fileout+'.vtp')
        writer.SetInputData(polydata)
        writer.Write()
        print('wrote into', fileout+'.vtp')
    elif method == 'vtkUnstructuredGrid':
        # caution: ParaView's Tube filter does not work on vtkUnstructuredGrid
        grid = vtk.vtkUnstructuredGrid()
        grid.SetPoints(points)
        if edges:
            grid.SetCells(vtk.VTK_LINE, line)
        if scalar:
            grid.GetPointData().AddArray(attribute0)
            if len(scalar) > 1:
                grid.GetPointData().AddArray(attribute1)
        if nodeLabel:
            grid.GetPointData().AddArray(label)
        writer = vtk.vtkXMLUnstructuredGridWriter()
        writer.SetFileName(fileout+'.vtu')
        writer.SetInputData(grid)
        writer.Write()
        print('wrote into', fileout+'.vtu')

import os

def segment_text_into_chunks(text, chunk_size=500):
    words = text.split()
    for i in range(0, len(words), chunk_size):
        yield ' '.join(words[i:i+chunk_size])

def process_text_files(folder_path):
    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)

        # Read file content
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().replace('\n', ' ')

        # Segment the text into chunks and prepare the modified content
        modified_content = [chunk for chunk in segment_text_into_chunks(content)]

        # Write the modified content back to the original file, using '\n\n' to separate paragraphs
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write('\n\n'.join(modified_content))

        # Count of paragraphs is the length of modified_content list
        paragraph_count = len(modified_content)
        print(f"{file_name}: {paragraph_count} paragraphs inserted.")

# Example usage
folder_path = '/content/input'
process_text_files(folder_path)

pip install nltk

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

# Ensure that NLTK's tokenizers and corpora are downloaded
nltk.download('punkt')

def segment_text_into_chunks(text, chunk_size=2000):
    """
    Segments the provided text into chunks of up to chunk_size words.
    """
    words = text.split()
    for i in range(0, len(words), chunk_size):
        yield ' '.join(words[i:i+chunk_size])

def calculate_ttr(text):
    """
    Calculates the Token-Type Ratio (TTR) of the given text using NLTK.
    """
    tokens = word_tokenize(text)
    types = set(tokens)
    return len(types) / len(tokens) if tokens else 0

def process_text_files(folder_path):
    """
    Processes text files in the given folder_path to calculate the TTR of each chunk.
    """
    # DataFrame to store the results
    results_df = pd.DataFrame()

    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)

        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().replace('\n', ' ')

        # Calculate TTR for each chunk
        ttr_values = [calculate_ttr(chunk) for chunk in segment_text_into_chunks(content)]

        # Create a temporary DataFrame to hold current file's TTR values
        temp_df = pd.DataFrame({file_name: ttr_values}).transpose()
        temp_df.columns = [f'Chunk {i+1}' for i in range(len(ttr_values))]

        # Append the results to the main DataFrame
        results_df = pd.concat([results_df, temp_df])

    # Adjust the DataFrame so document names are in the first column
    results_df.reset_index(inplace=True)
    results_df.rename(columns={'index': 'Document Name'}, inplace=True)

    return results_df

# Path to your text files
folder_path = '/content/input'
df_results = process_text_files(folder_path)
print(df_results)

# Correct the path to include the file name and extension
csv_file_path = '/content/TTR_table.csv'

# Save the DataFrame to a CSV file§
df_results.to_csv(csv_file_path, index=False)

# Now csv_file_path correctly points to the saved CSV file
print(csv_file_path)

"""#TTR Beowulf"""

pip install cltk

import os
import pandas as pd
from cltk import NLP

def segment_text_into_chunks(text, chunk_size=1000):
    """
    Segments the provided text into chunks of up to chunk_size words.
    """
    words = text.split()
    for i in range(0, len(words), chunk_size):
        yield ' '.join(words[i:i+chunk_size])

def calculate_ttr_cltk(text, cltk_nlp):
    """
    Calculates the Token-Type Ratio (TTR) of the given text using CLTK.
    """
    doc = cltk_nlp.analyze(text=text)
    # Since tokens are strings, use them directly
    tokens = [token for token in doc.tokens]
    types = set(tokens)
    return len(types) / len(tokens) if tokens else 0


def process_old_english_text_in_chunks(file_path, chunk_size=1000):
    """
    Processes an Old English text file to calculate the TTR of each chunk using CLTK,
    appending the results to a DataFrame.
    """
    cltk_nlp = NLP(language="ang")

    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    ttr_values = []
    for chunk in segment_text_into_chunks(text, chunk_size):
        ttr = calculate_ttr_cltk(chunk, cltk_nlp)
        ttr_values.append(ttr)

    # Convert TTR values to a DataFrame
    file_name = os.path.basename(file_path)
    temp_df = pd.DataFrame({file_name: ttr_values}).transpose()
    temp_df.columns = [f'Chunk {i+1}' for i in range(len(ttr_values))]

    return temp_df

def process_text_files(folder_path, include_old_english=False, old_english_file_path=None):
    """
    Processes text files in the given folder_path to calculate the TTR of each chunk,
    and optionally includes Old English text analysis.
    """
    results_df = pd.DataFrame()
    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)
        # Assuming you're applying a similar TTR calculation for these files as earlier
        # This part of the code might need adjustment

    if include_old_english and old_english_file_path:
        oe_df = process_old_english_text_in_chunks(old_english_file_path)
        results_df = pd.concat([results_df, oe_df])

    results_df.reset_index(inplace=True)
    results_df.rename(columns={'index': 'Document Name'}, inplace=True)

    return results_df

# Adjust the paths accordingly
folder_path = '/content/input'
old_english_file_path = '/content/beowulf_clean.txt'
df_results = process_text_files(folder_path, include_old_english=True, old_english_file_path=old_english_file_path)
print(df_results)

# Adjust the paths accordingly
folder_path = '/content/input'
old_english_file_path = '/content/beowulf_clean.txt'
df_results = process_text_files(folder_path, include_old_english=True, old_english_file_path=old_english_file_path)

# Correct the path to include the file name and extension
csv_file_path = '/content/TTR_table.csv'

# Check if the CSV file exists and read it if it does
if os.path.exists(csv_file_path):
    existing_df = pd.read_csv(csv_file_path)
    combined_df = pd.concat([existing_df, df_results], ignore_index=True)
else:
    combined_df = df_results

# Save the combined DataFrame to a CSV file, overwriting the old file
combined_df.to_csv(csv_file_path, index=False)

# Now csv_file_path correctly points to the saved CSV file
print(f"Data saved to {csv_file_path}")

"""#Diagrams"""

import pandas as pd
import plotly.graph_objects as go

# Load the DataFrame from the CSV file
csv_file_path = '/content/TTR_table.csv'
'''df_results = pd.read_csv(csv_file_path)'''

# Set 'Document Name' as the index and transpose for plotting
df_plot = df_results.set_index('Document Name').transpose()

# Create a Plotly figure
fig = go.Figure()

for document_name in df_plot.columns:
    # Extract TTR values for each document
    ttr_values = df_plot[document_name].values

    # Add a trace (line) for each document
    fig.add_trace(go.Scatter(x=df_plot.index,
                             y=ttr_values,
                             mode='lines+markers',
                             marker=dict(size=8),  # Adjust marker size
                             line=dict(width=2),  # Adjust line width
                             name=document_name))

# Update layout to match style settings
fig.update_layout(
    title={
        'text': 'Token-Type Ratio (TTR) by Chunk',
        'y':0.9,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': dict(size=20)
    },
    xaxis=dict(
        title='Chunk Number',
        titlefont_size=16,
        tickfont_size=14,
        tickangle=45
    ),
    yaxis=dict(
        title='Token-Type Ratio (TTR)',
        titlefont_size=16,
        tickfont_size=14
    ),
    legend=dict(
        title='Document Name',
        title_font_size=14,
        font_size=12,
        y=1.15,
        x=1.05,
        bgcolor='rgba(255, 255, 255, 0.5)',
        bordercolor='Black',
        borderwidth=1
    ),
    margin=dict(l=100, r=100, t=100, b=100),  # Adjust margins
    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
    width=2000,  # Figure size
    height=1000
)

# Adding grid lines for better readability
fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')
fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')

fig.show()

# Save the figure to an HTML file
fig.write_html("ttr_plot1.html")

import plotly.graph_objects as go

# Assuming df_results is your DataFrame and it's structured as before.
df_plot = df_results.set_index('Document Name').transpose()

# Create a Plotly figure
fig = go.Figure()

for document_name in df_plot.columns:
    # Extract TTR values for each document
    ttr_values = df_plot[document_name].values

    # Add a trace (line) for each document
    fig.add_trace(go.Scatter(x=df_plot.index,
                             y=ttr_values,
                             mode='lines+markers',
                             marker=dict(size=8),  # Adjust marker size
                             line=dict(width=2),  # Adjust line width
                             name=document_name))

# Update layout to match Matplotlib style settings
fig.update_layout(
    title={
        'text': 'Token-Type Ratio (TTR) by Chunk',
        'y':0.9,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': dict(size=20)
    },
    xaxis=dict(
        title='Chunk Number',
        titlefont_size=16,
        tickfont_size=14,
        tickangle=45
    ),
    yaxis=dict(
        title='Token-Type Ratio (TTR)',
        titlefont_size=16,
        tickfont_size=14
    ),
    legend=dict(
        title='Document Name',
        title_font_size=14,
        font_size=12,
        y=1.15,
        x=1.05,
        bgcolor='rgba(255, 255, 255, 0.5)',
        bordercolor='Black',
        borderwidth=1
    ),
    margin=dict(l=100, r=100, t=100, b=100),  # Adjust margins to fit
    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
    width=2000,  # Match the figure size to Matplotlib's figsize
    height=1000
)

# Adding grid lines for better readability
fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')
fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')

fig.show()

# Save the figure to an HTML file
fig.write_html("ttr_plot2.html")

"""#Vector Space"""

import os
import re
from collections import Counter

def aggregate_word_counts(folder_path):
    aggregated_counts = Counter()
    file_word_sets = []  # List to hold sets of words from each file

    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().lower()  # Convert to lower case for uniformity
            words = re.findall(r'\w+', content)
            aggregated_counts.update(words)
            file_word_sets.append(set(words))

    # Identify common words in all texts
    common_words = set.intersection(*file_word_sets)

    # Filter the aggregated_counts to only include common words
    common_word_counts = {word: count for word, count in aggregated_counts.items() if word in common_words}
    return common_word_counts

def top_words(word_counts):
    # Convert the dictionary to a Counter object and get the most common words
    return Counter(word_counts).most_common(500)

def process_texts(folder_path):
    # Aggregate word counts across all files
    common_word_counts = aggregate_word_counts(folder_path)

    # Get the top common words
    most_common_words = top_words(common_word_counts)

    # Print the result
    print("Top common words across all texts:")
    for index, (word, count) in enumerate(most_common_words, start=1):
        print(f"{index}. {word}: {count}")

# Usage
folder_path = '/content/input'
process_texts(folder_path)

def process_texts(folder_path):
    # Aggregate word counts across all files
    aggregated_counts = aggregate_word_counts(folder_path)

    # Calculate total tokens and types
    total_tokens = sum(aggregated_counts.values())
    total_types = len(aggregated_counts)

    # Print the results
    print(f"Total tokens: {total_tokens}")
    print(f"Total types: {total_types}")

# Usage
folder_path = '/content/input'
process_texts(folder_path)

import os
import re
from itertools import combinations
import numpy as np

def calculate_shared_words(folder_path):
    file_word_sets = {}  # Dictionary to hold sets of words from each file

    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    # Read and tokenize each file, storing the set of unique words
    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().lower()
            words = set(re.findall(r'\w+', content))
            file_word_sets[file_name] = words

    # Calculate shared words between every pair of files
    shared_words_counts = []
    for (file1, words1), (file2, words2) in combinations(file_word_sets.items(), 2):
        shared_words = len(words1.intersection(words2))
        shared_words_counts.append(shared_words)

    return shared_words_counts

def analyze_shared_words_counts(shared_words_counts):
    # Convert to numpy array for statistical analysis
    counts_array = np.array(shared_words_counts)

    # Calculate statistical measures
    average = np.mean(counts_array)
    median = np.median(counts_array)
    std_dev = np.std(counts_array)

    return average, median, std_dev

def process_and_analyze_texts(folder_path):
    shared_words_counts = calculate_shared_words(folder_path)
    average, median, std_dev = analyze_shared_words_counts(shared_words_counts)

    # Print the results
    print(f"Average number of shared words: {average:.2f}")
    print(f"Median number of shared words: {median:.2f}")
    print(f"Standard deviation: {std_dev:.2f}")

# Example usage
folder_path = '/content/input'
process_and_analyze_texts(folder_path)

import os
import re
import random

def extract_chunks(folder_path, chunk_size=100, chunks_needed=15):
    chunks = []

    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for file_name in txt_files:
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().lower()
            words = re.findall(r'\w+', content)
            for i in range(0, len(words), chunk_size):
                chunk = ' '.join(words[i:i+chunk_size])
                chunks.append(chunk)

    return random.sample(chunks, min(chunks_needed, len(chunks)))

# Usage
folder_path = '/content/input'
random_chunks = extract_chunks(folder_path)
for chunk in random_chunks:
    print(chunk + " \n")  # Added a newline and space for separation between chunks.

def calculate_accuracy_percentage(total_errors, total_words):
    accuracy_percentage = (1 - total_errors / total_words) * 100
    return accuracy_percentage

# Lists to store data for the texts
total_errors = [2, 1, 2, 3, 7, 1, 1, 2, 1, 1 , 3, 4, 1, 3 , 1]  # Fill in or append errors for each of the texts
total_words = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]  # Fill in or append word counts for each of the 15 texts

# Ensure both lists have the same length, corresponding to the texts
assert len(total_errors) == len(total_words) == 15

# Calculate and store accuracy for each text
accuracies = [calculate_accuracy_percentage(errors, words) for errors, words in zip(total_errors, total_words)]

# Calculate the average accuracy across all texts
average_accuracy = sum(accuracies) / len(accuracies)

# Print the results
for i, accuracy in enumerate(accuracies, start=1):
    print(f"Accuracy of text {i}: {accuracy}%")
print(f"Average Accuracy: {average_accuracy}%")

"""##Vectorise"""

import string
from gensim import corpora
from collections import defaultdict
from scipy.spatial import distance
import numpy as np
from sklearn import manifold
from tqdm import tqdm

npar = 61   # target number of paragraphs per text
size = []   # actual number of paragraphs per text

documents = readLongestParagraphs('/content/input/1849. Wackerbarth, Dietrich.txt', size, npar)    # 1 Wackerbarth
documents += readLongestParagraphs('/content/input/1855. Thorpe, Benjamin.txt', size, npar)  # 2 Thorpe
documents += readLongestParagraphs('/content/input/1876. Arnold, Thomas.txt', size, npar)   # 3 Arnold
documents += readLongestParagraphs('/content/input/1881. Lumsden, Henry W..txt', size, npar)   # 4 Lumsden
documents += readLongestParagraphs('/content/input/1882. Garnett.txt', size, npar)         # 5 Garnett
documents += readLongestParagraphs('/content/input/1897. Hall, John Lesslie.txt', size, npar) # 6 Hall, John Lesllie
documents += readLongestParagraphs('/content/input/1901. Hall, J. R. Clark (prose).txt', size, npar) # 7 Hall Clark (prose)
documents += readLongestParagraphs('/content/input/1910. Gummere, Francis B.txt', size, npar) # 8 Gummere
documents += readLongestParagraphs('/content/input/1910. Morris, Wyatt.txt', size, npar) # 9 1910 Morris
documents += readLongestParagraphs('/content/input/1913. Kirtlan E. J.txt', size, npar)  # 10 1913 Kirtlan
documents += readLongestParagraphs('/content/input/1914. Hall, J. R. Clark (metrical).txt', size, npar)  # 7 1914 Hall, J. R. Clark (verse)
documents += readLongestParagraphs('/content/input/1923. Gordon R. K.txt', size, npar) # 11 Gordon 1923
documents += readLongestParagraphs('/content/input/2014-1926. Tolkien, J. R. R..txt', size, npar)  # 12 Tolkien 1926/2014
documents += readLongestParagraphs('/content/input/1952. Edwin Morgan.txt', size, npar)  # 13 Morgan 1952
documents += readLongestParagraphs('/content/input/1957. Wright, David.txt', size, npar)  # 14 Wright 1957
documents += readLongestParagraphs('/content/input/1963. Burton, Raffel - Beowulf.txt', size, npar)  # 15 Raffell 1963
documents += readLongestParagraphs('/content/input/1966-2002. Talbot Donaldson.txt', size, npar) # 16  Talbot Donaldson 1966
documents += readLongestParagraphs('/content/input/1968. Garmonsway, George N.txt', size, npar)  # 17 Garmonsway 1968
documents += readLongestParagraphs('/content/input/1968. ODonoghue, Crossley-Holland.txt', size, npar)  # 18 O'Donoghue 1968,
documents += readLongestParagraphs('/content/input/1973. Alexander, Michael.txt', size, npar)  # 19 Alexander 1973
documents += readLongestParagraphs('/content/input/2006-1977. Chickering, Howell D.txt', size, npar)  # 20 Howell Chickering 1977/2006
documents += readLongestParagraphs('/content/input/1982. Greenfield, Stanley B.txt', size, npar)  # 21 Greenfield 1982
documents += readLongestParagraphs('/content/input/1983. Osborn, Marijane.txt', size, npar)  # 22 Osborn 1983
documents += readLongestParagraphs('/content/input/2013. Lehmann, Ruth P.M.txt', size, npar)  # 23 Lehmann 1988
documents += readLongestParagraphs('/content/input/1991. Rebsamen, Frederick.txt', size, npar)  # 24 Rebsamen 1991
documents += readLongestParagraphs('/content/input/1991. Romano, Tim.txt', size, npar)  # 25 Romano 1999
documents += readLongestParagraphs('/content/input/2000. Heaney, Seamus.txt', size, npar)  # 26 Heaney 2000
documents += readLongestParagraphs('/content/input/2007. Ringler, Dick.txt', size, npar)  # 27 Ringler 2007
documents += readLongestParagraphs('/content/input/2012. Meyer, Thomas.txt', size, npar)  # 28 Meyer 2012
documents += readLongestParagraphs('/content/input/2020. Maria Dahvana Headley.txt', size, npar)  # 29 Dahvana Headley 2020
documents += readLongestParagraphs('/content/input/2021. Abbott et al., Jean.txt', size, npar)  # 30 Abbot et al. 2021
documents += readLongestParagraphs('/content/input/2021. Mitchell, Stephen.txt', size, npar)  # 31 Stephen Mitchell 2021
documents += readLongestParagraphs('/content/input/2022. Markotić, Nicole.txt', size, npar)  # 32 Markotić 2022


translatorTags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
translationPerAuthorTags = [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
yearTags = [1849, 1855, 1876, 1881, 1882, 1897, 1901, 1910, 1910, 1913, 1914, 1923, 1926, 1952, 1957, 1963, 1966, 1968, 1968, 1973, 1977, 1982, 1983, 1988, 1991, 1999, 2000, 2007, 2012, 2020, 2021, 2021, 2022]

# convert line breaks and dashes to spaces, and remove punctuation
for i, p in enumerate(documents):
    tmp = p.replace('\n', ' ').replace('-',' ')
    for c in string.punctuation:
        tmp = tmp.replace(c,'')
        tmp = re.sub(r'\d+', '', tmp)
    documents[i] = tmp

# remove common words and tokenize (break into words)
stoplist = set('the of to his in he that a was with s for i him then on it from as by they not all their my men at through'.split())
texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]

# count words across all paragraphs
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1

# remove words that appear only once across all paragraphs
texts = [[token for token in text if frequency[token] > 1] for text in texts]

# build a dictionary of words from scratch (not related to above word count)
dictionary = corpora.Dictionary(texts)
# print(dictionary.token2id)   # print IDs of all words in the dictionary
nwords = len(dictionary.token2id)
print('built a global dictionary with', nwords, 'words')

# convert documents to sparse vectors containing tuples (wordID, wordCount);
# corpus is a list of paragraphs, each is a list of tuples
corpus = [dictionary.doc2bow(text) for text in texts]

# convert sparse vectors to full vectors of length nwords
n = sum(size)
fullCorpus = np.zeros((n,nwords), dtype=np.int32)
for i, d in enumerate(corpus):
    for word in d:
        id, count = word
        fullCorpus[i,id] = count

# connect pairs with at least n words in common
n, i = len(fullCorpus), -1
edges = []
for p1 in tqdm(fullCorpus):
    i += 1
    row = []
    for j, p2 in enumerate(fullCorpus):
        if i < j:
            if sum((p1!=0) * (p2!=0)) >= 100: # change number
                edges.append([i,j])

author, translationPerAuthor, year = [], [], []
for i, s in enumerate(size):
    author += [translatorTags[i]] * s
    translationPerAuthor += [translationPerAuthorTags[i]] * s
    year += [yearTags[i]] * s

import networkx as nx
H = nx.Graph()
H.add_nodes_from(range(n))
H.add_edges_from(edges)
pos = nx.spring_layout(H, k=0.9, dim=2)   # dictionary (nodeID,array([x,y,z]))
xyz = np.zeros((n,3))
for i in pos:
    xyz[i,:] = pos[i][0], pos[i][1], year[i]/2000.

print(nx.number_of_nodes(H), 'nodes and', nx.number_of_edges(H), 'edges')

print(xyz)
writeNodesEdges(xyz, edges=edges, scalar=[author, translationPerAuthor],
             name=['author','translation per author'], fileout='networkfile')

"""1.   test 1 : 1436 edges 0 nodes, threshold 330;
2.   test 2:  1634 edges 0 nodes, threshold 300;
3.   test 3:  1634 nodes and 4 edges, threshold 150;
4.   **test 4: 1634 nodes and 3613 edges;**
"""



